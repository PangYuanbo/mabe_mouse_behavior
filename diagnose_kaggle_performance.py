"""
è¯Šæ–­Kaggleæ€§èƒ½å·®è·çš„åˆ†æè„šæœ¬
åˆ†æè®­ç»ƒæ•°æ®ä¸æµ‹è¯•æ•°æ®çš„åˆ†å¸ƒå·®å¼‚ï¼Œä»¥åŠé—´éš”é¢„æµ‹è´¨é‡
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from collections import defaultdict, Counter
import torch
import sys

sys.path.insert(0, str(Path(__file__).parent))

from versions.v8_fine_grained.action_mapping import ID_TO_ACTION, NUM_ACTIONS
from versions.v8_fine_grained.submission_utils import predictions_to_intervals, evaluate_intervals


def analyze_training_data_distribution(data_dir):
    """åˆ†æè®­ç»ƒæ•°æ®çš„è¡Œä¸ºåˆ†å¸ƒ"""
    data_dir = Path(data_dir)
    train_annotation_dir = data_dir / 'train_annotation'
    
    if not train_annotation_dir.exists():
        print(f"è®­ç»ƒæ ‡æ³¨ç›®å½•æœªæ‰¾åˆ°: {train_annotation_dir}")
        return None
    
    # æ”¶é›†æ‰€æœ‰è®­ç»ƒæ ‡æ³¨æ•°æ®
    all_intervals = []
    lab_dirs = [d for d in train_annotation_dir.iterdir() if d.is_dir()]
    
    print(f"å‘ç° {len(lab_dirs)} ä¸ªå®éªŒå®¤æ•°æ®ç›®å½•")
    
    for lab_dir in lab_dirs:
        parquet_files = list(lab_dir.glob('*.parquet'))
        print(f"  {lab_dir.name}: {len(parquet_files)} ä¸ªè§†é¢‘")
        
        for parquet_file in parquet_files:
            try:
                df = pd.read_parquet(parquet_file)
                all_intervals.append(df)
            except Exception as e:
                print(f"    è·³è¿‡æ–‡ä»¶ {parquet_file}: {e}")
    
    if not all_intervals:
        print("æœªæ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
        return None
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    train_df = pd.concat(all_intervals, ignore_index=True)
    print(f"\nè®­ç»ƒæ•°æ®æ€»è®¡: {len(train_df)} ä¸ªé—´éš”")
    
    # è¡Œä¸ºåˆ†å¸ƒ
    action_counts = train_df['action'].value_counts()
    print(f"\nè®­ç»ƒæ•°æ®è¡Œä¸ºåˆ†å¸ƒ:")
    for action, count in action_counts.head(10).items():
        print(f"  {action}: {count} ({count/len(train_df)*100:.1f}%)")
    
    # é—´éš”é•¿åº¦åˆ†å¸ƒ
    train_df['duration'] = train_df['stop_frame'] - train_df['start_frame'] + 1
    
    print(f"\né—´éš”é•¿åº¦ç»Ÿè®¡:")
    print(f"  å¹³å‡é•¿åº¦: {train_df['duration'].mean():.1f} å¸§")
    print(f"  ä¸­ä½æ•°é•¿åº¦: {train_df['duration'].median():.1f} å¸§")
    print(f"  æœ€å°é•¿åº¦: {train_df['duration'].min()} å¸§")
    print(f"  æœ€å¤§é•¿åº¦: {train_df['duration'].max()} å¸§")
    print(f"  çŸ­é—´éš”(<5å¸§): {(train_df['duration'] < 5).sum()} ({(train_df['duration'] < 5).mean()*100:.1f}%)")
    print(f"  ä¸­ç­‰é—´éš”(5-30å¸§): {((train_df['duration'] >= 5) & (train_df['duration'] <= 30)).sum()}")
    print(f"  é•¿é—´éš”(>30å¸§): {(train_df['duration'] > 30).sum()}")
    
    return {
        'action_distribution': action_counts,
        'duration_stats': train_df['duration'].describe(),
        'total_intervals': len(train_df)
    }


def analyze_submission_quality(submission_path):
    """åˆ†ææäº¤æ–‡ä»¶çš„è´¨é‡"""
    if not Path(submission_path).exists():
        print(f"æäº¤æ–‡ä»¶æœªæ‰¾åˆ°: {submission_path}")
        return None
    
    submission_df = pd.read_csv(submission_path)
    print(f"\næäº¤æ–‡ä»¶åˆ†æ: {submission_path}")
    print(f"æ€»é¢„æµ‹é—´éš”æ•°: {len(submission_df)}")
    
    if len(submission_df) == 0:
        print("âš ï¸  æäº¤æ–‡ä»¶ä¸ºç©ºï¼è¿™æ˜¯ä¸»è¦é—®é¢˜ä¹‹ä¸€")
        return None
    
    # è¡Œä¸ºåˆ†å¸ƒ
    action_counts = submission_df['action'].value_counts()
    print(f"\næäº¤çš„è¡Œä¸ºåˆ†å¸ƒ:")
    for action, count in action_counts.items():
        print(f"  {action}: {count} ({count/len(submission_df)*100:.1f}%)")
    
    # é—´éš”é•¿åº¦åˆ†æ
    submission_df['duration'] = submission_df['stop_frame'] - submission_df['start_frame'] + 1
    
    print(f"\næäº¤é—´éš”é•¿åº¦ç»Ÿè®¡:")
    print(f"  å¹³å‡é•¿åº¦: {submission_df['duration'].mean():.1f} å¸§")
    print(f"  ä¸­ä½æ•°é•¿åº¦: {submission_df['duration'].median():.1f} å¸§")
    print(f"  æœ€å°é•¿åº¦: {submission_df['duration'].min()} å¸§")
    print(f"  æœ€å¤§é•¿åº¦: {submission_df['duration'].max()} å¸§")
    
    # è§†é¢‘è¦†ç›–åº¦
    unique_videos = submission_df['video_id'].nunique()
    print(f"\nè§†é¢‘è¦†ç›–:")
    print(f"  æ¶‰åŠè§†é¢‘æ•°: {unique_videos}")
    
    return {
        'total_predictions': len(submission_df),
        'action_distribution': action_counts,
        'duration_stats': submission_df['duration'].describe(),
        'unique_videos': unique_videos
    }


def simulate_interval_conversion():
    """æ¨¡æ‹Ÿå¸§çº§é¢„æµ‹åˆ°é—´éš”è½¬æ¢çš„å½±å“"""
    print("\n=== æ¨¡æ‹Ÿé—´éš”è½¬æ¢åˆ†æ ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ® - ç¢ç‰‡åŒ–é¢„æµ‹
    T = 1000
    action_preds = np.zeros(T, dtype=int)
    
    # æ¨¡æ‹Ÿä¸€äº›çœŸå®è¡Œä¸ºé—´éš”
    true_intervals = [
        (100, 150, 12),  # attack: 50å¸§
        (200, 210, 12),  # attack: 10å¸§  
        (300, 305, 13),  # chase: 5å¸§
        (400, 420, 19),  # avoid: 20å¸§
    ]
    
    for start, end, action in true_intervals:
        action_preds[start:end+1] = action
    
    # æ·»åŠ å™ªå£° - æ¨¡æ‹Ÿé¢„æµ‹ä¸ç¨³å®š
    for start, end, action in true_intervals:
        # éšæœºç§»é™¤ä¸€äº›å¸§
        noise_mask = np.random.random(end-start+1) < 0.2  # 20%å™ªå£°
        noise_indices = np.where(noise_mask)[0] + start
        action_preds[noise_indices] = 0
    
    agent_preds = np.random.randint(0, 4, T)
    target_preds = np.random.randint(0, 4, T)
    
    # æµ‹è¯•ä¸åŒå‚æ•°çš„å½±å“
    min_durations = [1, 3, 5, 10]
    confidence_thresholds = [0.3, 0.5, 0.7, 0.9]
    
    print(f"æ¨¡æ‹Ÿæ•°æ®: {len(true_intervals)} ä¸ªçœŸå®é—´éš”")
    
    for min_dur in min_durations:
        for conf_thresh in confidence_thresholds:
            intervals = predictions_to_intervals(
                action_preds, agent_preds, target_preds,
                min_duration=min_dur, confidence_threshold=conf_thresh
            )
            
            # è¿‡æ»¤æ‰background
            intervals = [i for i in intervals if i['action_id'] > 0]
            
            print(f"min_duration={min_dur:2d}, confidence={conf_thresh:.1f}: {len(intervals):2d} é—´éš”")


def test_interval_postprocessing():
    """æµ‹è¯•ä¸åŒåå¤„ç†ç­–ç•¥"""
    print("\n=== é—´éš”åå¤„ç†ç­–ç•¥æµ‹è¯• ===")
    
    # æ¨¡æ‹Ÿç¢ç‰‡åŒ–é¢„æµ‹
    intervals = [
        {'action_id': 12, 'agent_id': 0, 'target_id': 1, 'start_frame': 100, 'stop_frame': 105, 'confidence': 0.8},
        {'action_id': 12, 'agent_id': 0, 'target_id': 1, 'start_frame': 107, 'stop_frame': 110, 'confidence': 0.7},  # å°é—´éš™
        {'action_id': 12, 'agent_id': 0, 'target_id': 1, 'start_frame': 115, 'stop_frame': 120, 'confidence': 0.6},  # è¾ƒå¤§é—´éš™
        
        {'action_id': 13, 'agent_id': 1, 'target_id': 2, 'start_frame': 200, 'stop_frame': 201, 'confidence': 0.9},  # å¾ˆçŸ­
        {'action_id': 13, 'agent_id': 1, 'target_id': 2, 'start_frame': 203, 'stop_frame': 208, 'confidence': 0.8},
    ]
    
    print(f"åŸå§‹é—´éš”: {len(intervals)}")
    for i, interval in enumerate(intervals):
        duration = interval['stop_frame'] - interval['start_frame'] + 1
        print(f"  #{i}: {ID_TO_ACTION[interval['action_id']]} å¸§{interval['start_frame']}-{interval['stop_frame']} ({duration}å¸§)")
    
    # ç­–ç•¥1: åŸºæœ¬è¿‡æ»¤ (å½“å‰æ–¹æ³•)
    basic_filtered = [i for i in intervals if (i['stop_frame'] - i['start_frame'] + 1) >= 5]
    print(f"\nåŸºæœ¬è¿‡æ»¤(min_duration=5): {len(basic_filtered)} é—´éš”")
    
    # ç­–ç•¥2: æ™ºèƒ½åˆå¹¶
    merged = merge_nearby_intervals(intervals.copy(), gap_threshold=5)
    print(f"æ™ºèƒ½åˆå¹¶(gap<=5): {len(merged)} é—´éš”")
    for i, interval in enumerate(merged):
        duration = interval['stop_frame'] - interval['start_frame'] + 1
        print(f"  #{i}: {ID_TO_ACTION[interval['action_id']]} å¸§{interval['start_frame']}-{interval['stop_frame']} ({duration}å¸§)")


def merge_nearby_intervals(intervals, gap_threshold=5):
    """åˆå¹¶ç›¸è¿‘çš„åŒç±»é—´éš”"""
    if not intervals:
        return []
    
    # æŒ‰å¼€å§‹å¸§æ’åº
    intervals = sorted(intervals, key=lambda x: x['start_frame'])
    
    merged = []
    current = intervals[0].copy()
    
    for next_interval in intervals[1:]:
        # æ£€æŸ¥æ˜¯å¦ä¸ºåŒç±»é—´éš”ä¸”è·ç¦»è¾ƒè¿‘
        same_behavior = (
            current['action_id'] == next_interval['action_id'] and
            current['agent_id'] == next_interval['agent_id'] and
            current['target_id'] == next_interval['target_id']
        )
        
        gap = next_interval['start_frame'] - current['stop_frame'] - 1
        
        if same_behavior and gap <= gap_threshold:
            # åˆå¹¶
            current['stop_frame'] = next_interval['stop_frame']
            # æƒé‡å¹³å‡ç½®ä¿¡åº¦
            w1 = current['stop_frame'] - current['start_frame'] + 1
            w2 = next_interval['stop_frame'] - next_interval['start_frame'] + 1
            current['confidence'] = (current['confidence'] * w1 + next_interval['confidence'] * w2) / (w1 + w2)
        else:
            # ä¸èƒ½åˆå¹¶ï¼Œä¿å­˜å½“å‰é—´éš”
            merged.append(current)
            current = next_interval.copy()
    
    # æ·»åŠ æœ€åä¸€ä¸ªé—´éš”
    merged.append(current)
    
    return merged


def recommend_improvements():
    """ç»™å‡ºæ”¹è¿›å»ºè®®"""
    print("\n" + "="*60)
    print("ğŸ¯ Kaggleå¾—åˆ†æ”¹è¿›å»ºè®®")
    print("="*60)
    
    print("\n1. ğŸ“Š æ•°æ®åˆ†æå’ŒéªŒè¯")
    print("   - ä½¿ç”¨æ—¶é—´åˆ†å‰²è€Œééšæœºåˆ†å‰²åˆ›å»ºéªŒè¯é›†")
    print("   - åˆ†æè®­ç»ƒé›†ä¸­ä¸åŒè¡Œä¸ºçš„æ—¶é—´åˆ†å¸ƒç‰¹å¾")
    print("   - æ£€æŸ¥æµ‹è¯•é›†è§†é¢‘æ˜¯å¦æ¥è‡ªä¸åŒæ—¶é—´æ®µ/å®éªŒæ¡ä»¶")
    
    print("\n2. ğŸ”§ é—´éš”é¢„æµ‹ä¼˜åŒ–")
    print("   - é™ä½min_durationåˆ°3æˆ–æ›´å°")
    print("   - é™ä½confidence_thresholdåˆ°0.3æˆ–æ›´å°") 
    print("   - å®ç°æ™ºèƒ½é—´éš”åˆå¹¶ï¼Œå¤„ç†ç¢ç‰‡åŒ–é¢„æµ‹")
    
    print("\n3. ğŸ§  æ¨¡å‹æ¨ç†æ”¹è¿›")
    print("   - ä½¿ç”¨é‡å æ»‘åŠ¨çª—å£(50%é‡å )è¿›è¡Œensemble")
    print("   - å®ç°æµ‹è¯•æ—¶æ•°æ®å¢å¼º(TTA)")
    print("   - å¯¹çŸ­é—´éš”ä½¿ç”¨ä¸åŒçš„ç½®ä¿¡åº¦é˜ˆå€¼")
    
    print("\n4. ğŸ“ˆ åå¤„ç†ç­–ç•¥")
    print("   - åˆå¹¶é—´éš”è·ç¦»<=5å¸§çš„åŒç±»è¡Œä¸º")
    print("   - å¯¹ä¸åŒè¡Œä¸ºç±»åˆ«ä½¿ç”¨ä¸åŒçš„æœ€å°é•¿åº¦è¦æ±‚")
    print("   - åŸºäºè¡Œä¸ºç±»åˆ«çš„è‡ªé€‚åº”ç½®ä¿¡åº¦é˜ˆå€¼")
    
    print("\n5. ğŸ›ï¸ ç«‹å³å¯å°è¯•çš„å¿«é€Ÿä¿®å¤")
    print("   - ä¿®æ”¹inference_v8.pyä¸­çš„min_duration=3")
    print("   - åœ¨sliding windowä¸­æ·»åŠ 50%é‡å ")
    print("   - å®ç°é—´éš”åå¤„ç†åˆå¹¶")


def main():
    print("ğŸ” Kaggleæ€§èƒ½è¯Šæ–­åˆ†æ")
    print("="*60)
    
    # åˆ†æè®­ç»ƒæ•°æ®
    data_dir = "C:/Users/aaron/PycharmProjects/mabe_mouse_behavior/data/kaggle"
    train_stats = analyze_training_data_distribution(data_dir)
    
    # åˆ†ææäº¤æ–‡ä»¶
    submission_path = "submission_v8_local_test.csv"
    submission_stats = analyze_submission_quality(submission_path)
    
    # æ¨¡æ‹Ÿæµ‹è¯•
    simulate_interval_conversion()
    test_interval_postprocessing()
    
    # ç»™å‡ºå»ºè®®
    recommend_improvements()


if __name__ == "__main__":
    main()