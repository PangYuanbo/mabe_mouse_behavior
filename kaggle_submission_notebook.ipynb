{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# MABe Mouse Behavior Detection - Submission Notebook\n\n**Strategy:**\n- ✅ Use pre-trained Transformer model (old version)\n- ⚠️  No motion features (coordinates only)\n- ✅ Inference only (no training)\n- ⚡ Expected runtime: < 30 minutes\n\n**Setup Instructions:**\n1. Model already uploaded to: `mabe-submit` dataset\n2. Enable GPU (T4 or P100)\n3. Add dataset: `mabe-submit`\n4. Run all cells\n5. Submit `submission.csv`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MABe Kaggle Submission - Inference Only\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Define Model Architecture"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import math\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional encoding for transformer\"\"\"\n\n    def __init__(self, d_model, max_len=5000, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor of shape [seq_len, batch_size, embedding_dim]\n        \"\"\"\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\n\n\nclass TransformerBehaviorModel(nn.Module):\n    \"\"\"Transformer-based model for mouse behavior recognition\"\"\"\n\n    def __init__(self, input_dim, hidden_dim=256, num_layers=4,\n                 num_heads=8, num_classes=10, dropout=0.1, max_seq_len=256):\n        \"\"\"\n        Args:\n            input_dim: Dimension of input features (e.g., number of keypoints * 2)\n            hidden_dim: Dimension of hidden layers\n            num_layers: Number of transformer encoder layers\n            num_heads: Number of attention heads\n            num_classes: Number of behavior classes\n            dropout: Dropout rate\n            max_seq_len: Maximum sequence length\n        \"\"\"\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n\n        # Input projection\n        self.input_projection = nn.Linear(input_dim, hidden_dim)\n\n        # Positional encoding\n        self.pos_encoder = PositionalEncoding(hidden_dim, max_seq_len, dropout)\n\n        # Transformer encoder\n        encoder_layers = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=num_heads,\n            dim_feedforward=hidden_dim * 4,\n            dropout=dropout,\n            batch_first=False\n        )\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layers,\n            num_layers=num_layers\n        )\n\n        # Output layers\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, num_classes)\n        )\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: Input tensor of shape [batch_size, seq_len, input_dim]\n            mask: Optional mask tensor\n\n        Returns:\n            Output tensor of shape [batch_size, seq_len, num_classes]\n        \"\"\"\n        # Project input\n        x = self.input_projection(x)  # [batch, seq_len, hidden_dim]\n\n        # Transpose for transformer: [seq_len, batch, hidden_dim]\n        x = x.transpose(0, 1)\n\n        # Add positional encoding\n        x = self.pos_encoder(x)\n\n        # Apply transformer\n        x = self.transformer_encoder(x, src_key_padding_mask=mask)\n\n        # Transpose back: [batch, seq_len, hidden_dim]\n        x = x.transpose(0, 1)\n\n        # Classification\n        output = self.fc(x)\n\n        return output\n\nprint(\"✓ Model architecture defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Load Model Checkpoint"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\n\nif device.type == 'cuda':\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nprint()\n\n# Load checkpoint from mabe-submit dataset\nMODEL_PATH = Path('/kaggle/input/mabe-submit/best_model.pth')\n\ncheckpoint = torch.load(MODEL_PATH, map_location=device)\n\nprint(f\"✓ Loaded checkpoint from Epoch {checkpoint['epoch'] + 1}\")\nif 'best_val_f1' in checkpoint:\n    print(f\"  Best Val F1: {checkpoint['best_val_f1']:.4f}\")\nprint()\n\n# Build model - Old transformer (NO motion features, just coordinates)\nprint(\"⚠️  Using old model: coordinates only, no speed/acceleration\")\nmodel = TransformerBehaviorModel(\n    input_dim=144,  # 72 keypoints * 2 coords (NO motion features)\n    hidden_dim=256,\n    num_layers=4,\n    num_heads=8,\n    num_classes=4,\n    dropout=0.1,\n    max_seq_len=256\n)\n\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel = model.to(device)\nmodel.eval()\n\nprint(f\"✓ Model loaded\")\nprint(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Load Test Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DATA_DIR = Path('/kaggle/input/mabe-mouse-behavior-detection')\n\n# Load test metadata\nif (DATA_DIR / 'test.csv').exists():\n    test_csv = pd.read_csv(DATA_DIR / 'test.csv')\n    print(f\"Loaded test.csv with {len(test_csv)} videos\")\nelse:\n    # Fallback: use test labs from train.csv\n    train_csv = pd.read_csv(DATA_DIR / 'train.csv')\n    test_labs = ['MABe22_keypoints', 'MABe22_movies']\n    test_csv = train_csv[train_csv['lab_id'].isin(test_labs)].copy()\n    print(f\"Using test labs: {test_labs}\")\n\nprint(f\"Total test videos: {len(test_csv)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Generate Predictions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DATA_DIR = Path('/kaggle/input/mabe-mouse-behavior-detection')\n\n# Load test metadata\nif (DATA_DIR / 'test.csv').exists():\n    test_csv = pd.read_csv(DATA_DIR / 'test.csv')\n    print(f\"Loaded test.csv with {len(test_csv)} videos\")\nelse:\n    # Fallback: use test labs from train.csv\n    train_csv = pd.read_csv(DATA_DIR / 'train.csv')\n    test_labs = ['MABe22_keypoints', 'MABe22_movies']\n    test_csv = train_csv[train_csv['lab_id'].isin(test_labs)].copy()\n    print(f\"Using test labs: {test_labs}\")\n\nprint(f\"Total test videos: {len(test_csv)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Create Submission"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Generating predictions...\")\nprint()\n\nall_predictions = []\nsequence_length = 100\nstride = 25  # 75% overlap\n\nwith torch.no_grad():\n    for idx, row in tqdm(test_csv.iterrows(), total=len(test_csv), desc=\"Processing\"):\n        video_id = row['video_id']\n        lab_id = row['lab_id']\n        \n        # Load tracking\n        tracking_file = DATA_DIR / 'test_tracking' / lab_id / f'{video_id}.parquet'\n        if not tracking_file.exists():\n            continue\n        \n        try:\n            tracking_df = pd.read_parquet(tracking_file)\n            \n            # Convert to wide format\n            tracking_pivot = tracking_df.pivot_table(\n                index='video_frame',\n                columns=['mouse_id', 'bodypart'],\n                values=['x', 'y'],\n                aggfunc='first'\n            )\n            tracking_pivot.columns = ['_'.join(map(str, col)).strip() \n                                      for col in tracking_pivot.columns.values]\n            tracking_pivot = tracking_pivot.sort_index()\n            \n            keypoints = tracking_pivot.values.astype(np.float32)\n            keypoints = np.nan_to_num(keypoints, nan=0.0)\n            \n            # NO motion features for old model\n            num_frames = len(keypoints)\n            \n            # Accumulate predictions with sliding windows\n            video_preds = np.zeros((num_frames, 4), dtype=np.float32)\n            video_counts = np.zeros(num_frames, dtype=np.int32)\n            \n            for start_idx in range(0, max(1, num_frames - sequence_length + 1), stride):\n                end_idx = min(start_idx + sequence_length, num_frames)\n                \n                # Handle last window\n                if end_idx - start_idx < sequence_length:\n                    start_idx = max(0, num_frames - sequence_length)\n                    end_idx = num_frames\n                \n                window = keypoints[start_idx:end_idx]\n                \n                # Pad if needed\n                if len(window) < sequence_length:\n                    padding = np.zeros((sequence_length - len(window), window.shape[1]), \n                                       dtype=np.float32)\n                    window = np.concatenate([window, padding], axis=0)\n                \n                # Predict\n                window_tensor = torch.FloatTensor(window).unsqueeze(0).to(device)\n                output = model(window_tensor)\n                probs = torch.softmax(output, dim=-1).squeeze(0).cpu().numpy()\n                \n                # Accumulate\n                actual_length = min(sequence_length, end_idx - start_idx)\n                video_preds[start_idx:start_idx + actual_length] += probs[:actual_length]\n                video_counts[start_idx:start_idx + actual_length] += 1\n            \n            # Average overlapping predictions\n            video_counts = np.maximum(video_counts, 1)\n            video_preds = video_preds / video_counts[:, np.newaxis]\n            final_preds = np.argmax(video_preds, axis=1)\n            \n            # Create submission rows\n            for frame_idx, pred in enumerate(final_preds):\n                all_predictions.append({\n                    'video_id': video_id,\n                    'frame': frame_idx,\n                    'prediction': int(pred),\n                })\n        \n        except Exception as e:\n            print(f\"Error: {video_id} - {e}\")\n            continue\n\nprint(f\"\\n✓ Generated {len(all_predictions):,} predictions\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Create Submission"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(all_predictions)\n",
    "submission_df = submission_df.sort_values(['video_id', 'frame']).reset_index(drop=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Prediction Distribution\")\n",
    "print(\"=\"*60)\n",
    "class_names = {0: 'Background', 1: 'Social', 2: 'Mating', 3: 'Aggressive'}\n",
    "for class_id, count in submission_df['prediction'].value_counts().sort_index().items():\n",
    "    pct = count / len(submission_df) * 100\n",
    "    print(f\"{class_names[class_id]:12s}: {count:8,} ({pct:5.2f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save submission\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\n✓ Saved submission.csv\")\n",
    "print(f\"  Total predictions: {len(submission_df):,}\")\n",
    "print(f\"  Unique videos: {submission_df['video_id'].nunique()}\")\n",
    "print(\"\\nReady to submit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview submission\n",
    "submission_df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}