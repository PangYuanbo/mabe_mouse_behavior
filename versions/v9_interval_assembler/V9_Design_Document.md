# V9 é—´éš”ç»„è£…å™¨ (Interval Assembler) è®¾è®¡æ–‡æ¡£

## èƒŒæ™¯é—®é¢˜åˆ†æ

å½“å‰V8æ¨¡å‹å­˜åœ¨çš„æ ¸å¿ƒé—®é¢˜ï¼š
- **å¸§çº§å‡†ç¡®ç‡é«˜** (Action: 84%, Agent: 98%, Target: 90%)
- **é—´éš”çº§F1æä½** (Interval F1: 0.2075, Precision: 0.2212, Recall: 0.1955)
- **é¢„æµ‹è¿‡åº¦ç¢ç‰‡åŒ–**ï¼šæ¨¡å‹å€¾å‘äºé¢„æµ‹èƒŒæ™¯ç±»ï¼ŒçœŸæ­£çš„è¡Œä¸ºé—´éš”è¢«åˆ†å‰²
- **è¾¹ç•Œå®šä½ä¸å‡†**ï¼šå¸§çº§é¢„æµ‹è½¬æ¢ä¸ºé—´éš”æ—¶è¾¹ç•Œè¯¯å·®ç´¯ç§¯
- **è§„åˆ™å¼åå¤„ç†å±€é™**ï¼šå›ºå®šé˜ˆå€¼å’Œåˆå¹¶ç­–ç•¥æ— æ³•é€‚åº”ä¸åŒè¡Œä¸ºçš„æ—¶åºç‰¹æ€§

## V9è®¾è®¡ç†å¿µ

**æ ¸å¿ƒæ€æƒ³**ï¼šå¼•å…¥ä¸“é—¨çš„"é—´éš”é‡ç»„å™¨"ï¼Œå°†å¸§çº§é¢„æµ‹æ™ºèƒ½åœ°ç»„è£…æˆKaggleè¦æ±‚çš„é—´éš”æ ¼å¼ã€‚

**å…³é”®ä¼˜åŠ¿**ï¼š
1. **ç«¯åˆ°ç«¯ä¼˜åŒ–**ï¼šç›´æ¥é’ˆå¯¹Interval F1ä¼˜åŒ–ï¼Œè€Œéé—´æ¥çš„å¸§çº§å‡†ç¡®ç‡
2. **å¯å­¦ä¹ è¾¹ç•Œ**ï¼šå­¦ä¹ ä¸åŒè¡Œä¸ºçš„æœ€ä¼˜è¾¹ç•Œå®šä½ç­–ç•¥
3. **æ™ºèƒ½åˆå¹¶**ï¼šè‡ªåŠ¨å­¦ä¹ ä½•æ—¶åˆå¹¶ç›¸é‚»ç‰‡æ®µï¼Œä½•æ—¶ä¿æŒåˆ†å‰²
4. **è¡Œä¸ºè‡ªé€‚åº”**ï¼šä¸åŒè¡Œä¸ºä½¿ç”¨ä¸åŒçš„æ—¶é•¿å’Œç½®ä¿¡åº¦è¦æ±‚

---

## æ–¹æ¡ˆä¸€ï¼šå¯å­¦ä¹ é‡ç»„å™¨ (Learnable Assembler) ğŸš€

**ä¼˜å…ˆçº§**: â­â­â­â­â­ (ç«‹å³å®æ–½)

### æ¶æ„è®¾è®¡

```
è¾“å…¥: V8å¸§çº§logits [T, 28] + agent_logits [T, 4] + target_logits [T, 4] + åŸå§‹ç‰¹å¾ [T, 112]
      â†“
æ—¶åºç¼–ç å™¨: TCN/BiLSTM [T, 256]
      â†“
å¤šå¤´è¾“å‡º:
â”œâ”€â”€ Startè¾¹ç•Œæ£€æµ‹: [T, 28Ã—12] (28è¡Œä¸º Ã— 12å®šå‘é¼ å¯¹)
â”œâ”€â”€ Endè¾¹ç•Œæ£€æµ‹:   [T, 28Ã—12] 
â”œâ”€â”€ æ®µå†…ç½®ä¿¡åº¦:     [T, 28Ã—12]
â””â”€â”€ æ®µçº§ç‰¹å¾:      [T, 128] (ç”¨äºåç»­NMS)
```

### å…³é”®åˆ›æ–°ç‚¹

#### 1. å®šå‘é¼ å¯¹å»ºæ¨¡
- **12ä¸ªå®šå‘é¼ å¯¹**: mouse1â†’mouse2, mouse1â†’mouse3, mouse1â†’mouse4, mouse2â†’mouse1, ...
- **é¿å…agent/targetæ··ä¹±**: ç›´æ¥ä¸ºæ¯ä¸ª(action, agent, target)ç»„åˆå»ºæ¨¡
- **å¯¹ç§°æ€§å¤„ç†**: å¯¹reciprocalsniffç­‰åŒå‘è¡Œä¸ºç‰¹æ®Šå¤„ç†

#### 2. è½¯è¾¹ç•Œæ ‡ç­¾ç”Ÿæˆ
```python
def generate_soft_boundary_labels(gt_intervals, sequence_length, sigma=2):
    """ç”Ÿæˆé«˜æ–¯å¹³æ»‘çš„è¾¹ç•Œæ ‡ç­¾ï¼Œå‡å°‘è¾¹ç•Œå™ªå£°"""
    start_labels = np.zeros((sequence_length, 28*12))
    end_labels = np.zeros((sequence_length, 28*12))
    
    for interval in gt_intervals:
        action_id = interval['action_id']
        pair_id = get_pair_id(interval['agent_id'], interval['target_id'])
        channel = action_id * 12 + pair_id
        
        # é«˜æ–¯å¹³æ»‘è¾¹ç•Œ
        start_frame = interval['start_frame']
        end_frame = interval['stop_frame']
        
        start_labels[:, channel] += gaussian_kernel(range(sequence_length), start_frame, sigma)
        end_labels[:, channel] += gaussian_kernel(range(sequence_length), end_frame, sigma)
    
    return start_labels, end_labels
```

#### 3. å¤šä»»åŠ¡æŸå¤±å‡½æ•°
```python
class AssemblerLoss(nn.Module):
    def forward(self, pred_start, pred_end, pred_conf, gt_start, gt_end, gt_intervals):
        # 1. è¾¹ç•Œæ£€æµ‹æŸå¤± (Focal Losså¤„ç†ç±»åˆ«ä¸å¹³è¡¡)
        boundary_loss = focal_loss(pred_start, gt_start) + focal_loss(pred_end, gt_end)
        
        # 2. æ®µçº§Soft-IoUæŸå¤±
        pred_intervals = decode_intervals(pred_start, pred_end, pred_conf)
        soft_iou_loss = compute_soft_iou_loss(pred_intervals, gt_intervals)
        
        # 3. åç¢ç‰‡åŒ–æ­£åˆ™
        fragment_penalty = count_fragments_penalty(pred_intervals)
        
        return boundary_loss + 0.5 * soft_iou_loss + 0.1 * fragment_penalty
```

### æ¨ç†ç®¡é“

#### 1. å³°å€¼æ£€æµ‹ä¸é…å¯¹
```python
def decode_intervals(start_heatmap, end_heatmap, confidence):
    intervals = []
    
    for action_id in range(28):
        if action_id == 0:  # è·³è¿‡èƒŒæ™¯
            continue
            
        for pair_id in range(12):
            channel = action_id * 12 + pair_id
            
            # æ‰¾å³°å€¼
            start_peaks = find_peaks(start_heatmap[:, channel], min_conf=0.3)
            end_peaks = find_peaks(end_heatmap[:, channel], min_conf=0.3)
            
            # è´ªå¿ƒé…å¯¹
            for start_frame, start_conf in start_peaks:
                best_end = find_best_end(end_peaks, start_frame, max_duration=200)
                if best_end:
                    end_frame, end_conf = best_end
                    intervals.append({
                        'start_frame': start_frame,
                        'stop_frame': end_frame,
                        'action_id': action_id,
                        'agent_id': pair_id // 3,
                        'target_id': (pair_id % 3) + (1 if pair_id % 3 >= pair_id // 3 else 0),
                        'confidence': (start_conf + end_conf) / 2
                    })
    
    return intervals
```

#### 2. è¡Œä¸ºè‡ªé€‚åº”è¿‡æ»¤
```python
BEHAVIOR_CONFIGS = {
    'bite': {'min_duration': 2, 'conf_threshold': 0.4},
    'flinch': {'min_duration': 2, 'conf_threshold': 0.4},
    'ejaculate': {'min_duration': 3, 'conf_threshold': 0.3},
    'sniff': {'min_duration': 5, 'conf_threshold': 0.2},
    'chase': {'min_duration': 8, 'conf_threshold': 0.3},
    'attack': {'min_duration': 4, 'conf_threshold': 0.3},
    # ... å…¶ä»–è¡Œä¸º
}

def adaptive_filter(intervals):
    filtered = []
    for interval in intervals:
        action_name = ID_TO_ACTION[interval['action_id']]
        config = BEHAVIOR_CONFIGS.get(action_name, {'min_duration': 5, 'conf_threshold': 0.3})
        
        duration = interval['stop_frame'] - interval['start_frame'] + 1
        if duration >= config['min_duration'] and interval['confidence'] >= config['conf_threshold']:
            filtered.append(interval)
    
    return filtered
```

#### 3. æ—¶åºNMSä¸å°ç©ºæ´åˆå¹¶
```python
def temporal_nms_and_merge(intervals, nms_iou_threshold=0.3, merge_gap_threshold=5):
    # æŒ‰ç½®ä¿¡åº¦æ’åº
    intervals = sorted(intervals, key=lambda x: x['confidence'], reverse=True)
    
    # NMSå»é‡å 
    keep = []
    while intervals:
        best = intervals.pop(0)
        keep.append(best)
        
        # ç§»é™¤ä¸besté‡å åº¦é«˜çš„é—´éš”
        intervals = [itv for itv in intervals 
                    if compute_temporal_iou(best, itv) < nms_iou_threshold
                    or not same_behavior(best, itv)]
    
    # å°ç©ºæ´åˆå¹¶
    merged = merge_nearby_intervals(keep, gap_threshold=merge_gap_threshold)
    
    return merged
```

### è®­ç»ƒç­–ç•¥

#### æ•°æ®ç”Ÿæˆ
- **è¾“å…¥**: V8çš„å¸§çº§é¢„æµ‹ + åŸå§‹å…³é”®ç‚¹ç‰¹å¾
- **æ ‡ç­¾**: ä»train_annotationç”Ÿæˆçš„è½¯è¾¹ç•Œçƒ­åŠ›å›¾ + é—´éš”åˆ—è¡¨
- **å¢å¼º**: æ—¶é—´æŠ–åŠ¨ã€å™ªå£°æ³¨å…¥ã€ç‰‡æ®µè£å‰ª

#### ä¼˜åŒ–ç›®æ ‡
1. **ä¸»è¦ç›®æ ‡**: æœ€å¤§åŒ–Interval F1 (é€šè¿‡Soft-IoUé€¼è¿‘)
2. **è¾…åŠ©ç›®æ ‡**: è¾¹ç•Œç²¾åº¦ã€ç½®ä¿¡åº¦æ ¡å‡†
3. **æ­£åˆ™åŒ–**: åç¢ç‰‡åŒ–ã€ç±»åˆ«å¹³è¡¡

---

## æ–¹æ¡ˆäºŒï¼šAnchorå¼é—´éš”æ£€æµ‹å™¨ (Anchor-based Interval Detector) ğŸ¯

**ä¼˜å…ˆçº§**: â­â­â­â­ (ç¬¬äºŒé˜¶æ®µå®æ–½)

### æ¶æ„è®¾è®¡

```
è¾“å…¥: åŸå§‹ç‰¹å¾ [T, 112] + V8å¸§çº§logits [T, 28+4+4] (å¯é€‰è’¸é¦)
      â†“
Backbone: 1D ResNet + BiLSTM/Transformer
      â†“
Feature Pyramid: [T/1, T/2, T/4] å¤šå°ºåº¦ç‰¹å¾
      â†“
å¤šå°ºåº¦Anchor: 5,10,20,40,80,160,320å¸§ Ã— æ¯ä¸ªä½ç½®
      â†“
æ£€æµ‹å¤´:
â”œâ”€â”€ Actionåˆ†ç±»: [N_anchors, 28]
â”œâ”€â”€ Agent/Targetåˆ†ç±»: [N_anchors, 4+4] 
â”œâ”€â”€ Objectness: [N_anchors, 1]
â””â”€â”€ è¾¹ç•Œå›å½’: [N_anchors, 2] (start_offset, end_offset)
```

### å…³é”®æŠ€æœ¯

#### 1. å¤šå°ºåº¦Anchorç­–ç•¥
```python
class AnchorGenerator:
    def __init__(self, scales=[5, 10, 20, 40, 80, 160, 320], ratios=[0.5, 1.0, 2.0]):
        self.scales = scales
        self.ratios = ratios
    
    def generate_anchors(self, feature_length):
        anchors = []
        for pos in range(feature_length):
            for scale in self.scales:
                for ratio in self.ratios:
                    duration = int(scale * ratio)
                    center = pos * self.stride
                    start = center - duration // 2
                    end = center + duration // 2
                    anchors.append([start, end, duration])
        return torch.tensor(anchors)
```

#### 2. Anchoråˆ†é…ç­–ç•¥
```python
def assign_anchors(anchors, gt_intervals, pos_iou_threshold=0.5, neg_iou_threshold=0.1):
    ious = compute_temporal_iou_matrix(anchors, gt_intervals)
    
    # æ­£æ ·æœ¬: IoU >= 0.5
    positive_mask = ious.max(dim=1)[0] >= pos_iou_threshold
    
    # è´Ÿæ ·æœ¬: IoU <= 0.1  
    negative_mask = ious.max(dim=1)[0] <= neg_iou_threshold
    
    # å¿½ç•¥æ ·æœ¬: 0.1 < IoU < 0.5
    ignore_mask = ~positive_mask & ~negative_mask
    
    return positive_mask, negative_mask, ignore_mask
```

#### 3. è¾¹ç•Œå›å½’
```python
def encode_bbox_targets(anchors, gt_intervals):
    """å°†GTç¼–ç ä¸ºç›¸å¯¹anchorçš„åç§»"""
    # å½’ä¸€åŒ–åç§»
    anchor_centers = (anchors[:, 0] + anchors[:, 1]) / 2
    anchor_durations = anchors[:, 1] - anchors[:, 0]
    
    gt_centers = (gt_intervals[:, 0] + gt_intervals[:, 1]) / 2
    gt_durations = gt_intervals[:, 1] - gt_intervals[:, 0]
    
    # ç¼–ç ä¸ºç›¸å¯¹åç§»
    start_offsets = (gt_intervals[:, 0] - anchors[:, 0]) / anchor_durations
    end_offsets = (gt_intervals[:, 1] - anchors[:, 1]) / anchor_durations
    
    return torch.stack([start_offsets, end_offsets], dim=1)
```

### è®­ç»ƒæŸå¤±
```python
class AnchorBasedLoss(nn.Module):
    def forward(self, predictions, targets):
        # 1. åˆ†ç±»æŸå¤± (Focal Loss)
        action_loss = focal_loss(pred_actions, gt_actions, alpha=0.25, gamma=2.0)
        agent_loss = focal_loss(pred_agents, gt_agents)
        target_loss = focal_loss(pred_targets, gt_targets)
        
        # 2. ObjectnessæŸå¤±
        objectness_loss = binary_cross_entropy(pred_objectness, gt_objectness)
        
        # 3. è¾¹ç•Œå›å½’æŸå¤± (Smooth L1)
        regression_loss = smooth_l1_loss(pred_offsets, gt_offsets)
        
        # 4. æ®µçº§IoUæŸå¤± (åªå¯¹æ­£æ ·æœ¬)
        decoded_intervals = decode_predictions(predictions)
        iou_loss = diou_loss(decoded_intervals, gt_intervals)
        
        return action_loss + agent_loss + target_loss + objectness_loss + regression_loss + 0.5 * iou_loss
```

---

## æ–¹æ¡ˆä¸‰ï¼šç»“æ„åŒ–åºåˆ—è§£ç  (Structured Sequence Decoding) ğŸ§ 

**ä¼˜å…ˆçº§**: â­â­â­ (ç¬¬ä¸‰é˜¶æ®µå¢å¼º)

### åŠç›‘ç£CRFæ–¹æ³•

#### 1. Segment-level CRF
```python
class SegmentCRF(nn.Module):
    def __init__(self, num_actions, num_pairs):
        self.num_actions = num_actions
        self.num_pairs = num_pairs
        
        # è½¬ç§»çŸ©é˜µ: ä»çŠ¶æ€iè½¬ç§»åˆ°çŠ¶æ€jçš„ä»£ä»·
        self.transition_matrix = nn.Parameter(torch.randn(num_actions * num_pairs, num_actions * num_pairs))
        
        # æŒç»­æ—¶é•¿å…ˆéªŒ
        self.duration_model = DurationModel(num_actions)
    
    def forward(self, emission_scores, sequence_length):
        # emission_scores: [T, num_actions * num_pairs]
        # ä½¿ç”¨Viterbiç®—æ³•æ‰¾æœ€ä¼˜è·¯å¾„
        optimal_path = self.viterbi_decode(emission_scores, sequence_length)
        return self.path_to_intervals(optimal_path)
```

#### 2. å­¦ä¹ æ—¶é•¿åˆ†å¸ƒ
```python
class DurationModel(nn.Module):
    def __init__(self, num_actions):
        # ä¸ºæ¯ä¸ªè¡Œä¸ºå­¦ä¹ Gammaåˆ†å¸ƒå‚æ•°
        self.alpha = nn.Parameter(torch.ones(num_actions) * 2.0)
        self.beta = nn.Parameter(torch.ones(num_actions) * 0.1)
    
    def log_prob(self, duration, action_id):
        # è®¡ç®—åœ¨ç»™å®šè¡Œä¸ºä¸‹ï¼Œç‰¹å®šæ—¶é•¿çš„å¯¹æ•°æ¦‚ç‡
        return torch.distributions.Gamma(self.alpha[action_id], self.beta[action_id]).log_prob(duration)
```

### å­¦ä¹ åˆå¹¶å†³ç­–

```python
class MergeDecisionNetwork(nn.Module):
    def __init__(self, feature_dim=64):
        self.feature_extractor = nn.Sequential(
            nn.Linear(10, 64),  # 10ç»´ç»Ÿè®¡ç‰¹å¾
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
    
    def extract_pair_features(self, seg1, seg2):
        """æå–ç›¸é‚»ç‰‡æ®µå¯¹çš„ç‰¹å¾"""
        features = [
            seg1['duration'], seg2['duration'],
            seg1['confidence'], seg2['confidence'],
            seg2['start_frame'] - seg1['stop_frame'],  # gap
            int(seg1['action_id'] == seg2['action_id']),  # åŒè¡Œä¸º
            int(seg1['agent_id'] == seg2['agent_id']),    # åŒagent  
            int(seg1['target_id'] == seg2['target_id']),  # åŒtarget
            abs(seg1['confidence'] - seg2['confidence']),  # ç½®ä¿¡åº¦å·®
            min(seg1['duration'], seg2['duration']) / max(seg1['duration'], seg2['duration'])  # æ—¶é•¿æ¯”
        ]
        return torch.tensor(features, dtype=torch.float32)
    
    def should_merge(self, seg1, seg2):
        features = self.extract_pair_features(seg1, seg2)
        merge_prob = self.feature_extractor(features)
        return merge_prob > 0.5
```

---

## å…³é”®å·¥ç¨‹ç»†èŠ‚

### 1. é¼ å¯¹å®šå‘å»ºæ¨¡
```python
def get_pair_id(agent_id, target_id):
    """å°†(agent, target)æ˜ å°„åˆ°0-11çš„ID"""
    if agent_id == target_id:
        raise ValueError("Agent and target cannot be the same")
    
    # 4Ã—3çš„æ˜ å°„ (æ’é™¤å¯¹è§’çº¿)
    if target_id > agent_id:
        target_id -= 1  # è°ƒæ•´ç´¢å¼•
    
    return agent_id * 3 + target_id

def get_agent_target_from_pair_id(pair_id):
    """é€†æ˜ å°„ï¼šä»pair_idæ¢å¤(agent_id, target_id)"""
    agent_id = pair_id // 3
    target_offset = pair_id % 3
    target_id = target_offset if target_offset < agent_id else target_offset + 1
    return agent_id, target_id
```

### 2. è¾¹ç•Œé²æ£’æ€§å¢å¼º
```python
def gaussian_boundary_smoothing(labels, sigma=2):
    """å¯¹è¾¹ç•Œæ ‡ç­¾è¿›è¡Œé«˜æ–¯å¹³æ»‘ï¼Œå¢å¼ºé²æ£’æ€§"""
    smoothed = np.zeros_like(labels)
    kernel_size = 6 * sigma + 1
    kernel = np.exp(-0.5 * ((np.arange(kernel_size) - 3*sigma) / sigma)**2)
    kernel = kernel / kernel.sum()
    
    for i in range(labels.shape[1]):
        smoothed[:, i] = np.convolve(labels[:, i], kernel, mode='same')
    
    return smoothed

def soft_iou_loss(pred_intervals, gt_intervals, sigma=5.0):
    """å¯å¾®åˆ†çš„Soft-IoUæŸå¤±ï¼Œé€¼è¿‘0.5é˜ˆå€¼æ•ˆæœ"""
    total_loss = 0
    for pred in pred_intervals:
        best_iou = 0
        for gt in gt_intervals:
            if pred['action_id'] == gt['action_id'] and \
               pred['agent_id'] == gt['agent_id'] and \
               pred['target_id'] == gt['target_id']:
                # è®¡ç®—soft IoU
                intersection = torch.min(pred['end'], gt['end']) - torch.max(pred['start'], gt['start'])
                intersection = torch.clamp(intersection, min=0)
                union = (pred['end'] - pred['start']) + (gt['end'] - gt['start']) - intersection
                soft_iou = intersection / (union + 1e-6)
                best_iou = torch.max(best_iou, soft_iou)
        
        # é¼“åŠ±IoUæ¥è¿‘0.5ä»¥ä¸Š
        total_loss += torch.clamp(0.5 - best_iou, min=0)**2
    
    return total_loss / len(pred_intervals)
```

### 3. è‡ªé€‚åº”é˜ˆå€¼å­¦ä¹ 
```python
class AdaptiveThresholdModule(nn.Module):
    def __init__(self, num_actions):
        super().__init__()
        # ä¸ºæ¯ä¸ªè¡Œä¸ºå­¦ä¹ æœ€ä¼˜é˜ˆå€¼
        self.min_duration_logits = nn.Parameter(torch.log(torch.tensor([5.0] * num_actions)))
        self.conf_threshold_logits = nn.Parameter(torch.log(torch.tensor([0.3] * num_actions)))
        self.nms_threshold_logits = nn.Parameter(torch.log(torch.tensor([0.3] * num_actions)))
    
    def get_thresholds(self, action_id):
        return {
            'min_duration': torch.exp(self.min_duration_logits[action_id]),
            'conf_threshold': torch.sigmoid(self.conf_threshold_logits[action_id]),
            'nms_threshold': torch.sigmoid(self.nms_threshold_logits[action_id])
        }
```

---

## è½åœ°å®æ–½è®¡åˆ’

### ç¬¬1å‘¨ï¼šV9-AssembleråŸå‹ ğŸš€
- [ ] åˆ›å»ºè®­ç»ƒæ•°æ®ç”Ÿæˆç®¡é“ï¼ˆè½¯è¾¹ç•Œæ ‡ç­¾ï¼‰
- [ ] å®ç°TCN/BiLSTMé‡ç»„å™¨
- [ ] è®­ç»ƒè¾¹ç•Œæ£€æµ‹ + Soft-IoUå¤šä»»åŠ¡æŸå¤±
- [ ] å®ç°è‡ªé€‚åº”è§£ç ä¸NMS
- [ ] åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°Interval F1æå‡

### ç¬¬2-3å‘¨ï¼šV9-Proposalå®Œæ•´ç‰ˆ ğŸ¯  
- [ ] å¼•å…¥å¤šå°ºåº¦anchorå’ŒFPNç‰¹å¾
- [ ] åŠ å…¥æˆå¯¹å…³ç³»ç‰¹å¾ï¼ˆè·ç¦»ã€é€Ÿåº¦ã€æœå‘ï¼‰
- [ ] å®ç°anchoråˆ†é…å’Œè¾¹ç•Œå›å½’
- [ ] è’¸é¦V8å¸§çº§çŸ¥è¯†
- [ ] å®Œå–„è§£ç ç®¡é“å’Œé˜ˆå€¼è‡ªé€‚åº”

### ç¬¬4å‘¨ï¼šç»“æ„åŒ–è§£ç å¢å¼º ğŸ§ 
- [ ] å®ç°segment-level CRF
- [ ] è®­ç»ƒåˆå¹¶å†³ç­–ç½‘ç»œ
- [ ] é›†æˆæ—¶é•¿å…ˆéªŒæ¨¡å‹
- [ ] ç«¯åˆ°ç«¯å¾®è°ƒæ‰€æœ‰æ¨¡å—

### é¢„æœŸæ•ˆæœ
- **ç¬¬1å‘¨å**: Interval F1 ä» 0.21 â†’ 0.45+
- **ç¬¬3å‘¨å**: Interval F1 è¾¾åˆ° 0.60+
- **ç¬¬4å‘¨å**: Interval F1 è¾¾åˆ° 0.70+ï¼Œæ¥è¿‘SOTAæ°´å¹³

---

## æ€»ç»“

V9çš„æ ¸å¿ƒç†å¿µæ˜¯**"å¸§çº§å‡†ç¡®ç‡å·²ç»å¤Ÿå¥½ï¼Œå…³é”®åœ¨äºå¦‚ä½•æ™ºèƒ½ç»„è£…"**ã€‚é€šè¿‡å¼•å…¥ä¸“é—¨çš„é—´éš”é‡ç»„å™¨ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š

1. **ç›´æ¥ä¼˜åŒ–ç›®æ ‡æŒ‡æ ‡**ï¼šä¸å†ä¾èµ–å¸§çº§å‡†ç¡®ç‡çš„é—´æ¥æå‡
2. **å­¦ä¹ è¡Œä¸ºæ—¶åºç‰¹æ€§**ï¼šä¸åŒè¡Œä¸ºçš„è¾¹ç•Œã€æ—¶é•¿ã€ç½®ä¿¡åº¦è¦æ±‚
3. **ç«¯åˆ°ç«¯å¯å¾®åˆ†**ï¼šä»ç‰¹å¾åˆ°æœ€ç»ˆInterval F1çš„å®Œæ•´æ¢¯åº¦æµ
4. **å·¥ç¨‹åŒ–å‹å¥½**ï¼šåˆ†é˜¶æ®µå®æ–½ï¼Œé£é™©å¯æ§ï¼Œå¿«é€Ÿè¿­ä»£

è¿™ä¸ªè®¾è®¡æ—¢ä¿ç•™äº†V8çš„å¼ºå¤§å¸§çº§èƒ½åŠ›ï¼Œåˆè§£å†³äº†é—´éš”è½¬æ¢çš„æ ¸å¿ƒç—›ç‚¹ï¼Œæ˜¯å½“å‰æœ€æœ‰å¸Œæœ›å¤§å¹…æå‡Kaggleå¾—åˆ†çš„æ–¹æ¡ˆã€‚