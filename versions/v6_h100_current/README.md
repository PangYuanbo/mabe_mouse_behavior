# V6 - H100è¶…é€Ÿè®­ç»ƒç‰ˆæœ¬ âš¡

## ğŸ“‹ ç‰ˆæœ¬æ¦‚è¿°

**ç‰ˆæœ¬å·**: V6
**å¼€å‘æ—¶é—´**: V5ä¹‹å
**çŠ¶æ€**: **å½“å‰ç‰ˆæœ¬ï¼ˆç”Ÿäº§ä¸­ï¼‰**
**ç›®æ ‡**: H100æè‡´åŠ é€Ÿï¼Œ1.4å°æ—¶å®Œæˆ100 epochsè®­ç»ƒ

---

## ğŸ¯ è®¾è®¡æ€æƒ³

### æ ¸å¿ƒç›®æ ‡
1. ä½¿ç”¨H100 (80GB)å®ç°è®­ç»ƒæè‡´åŠ é€Ÿ
2. å¤§batchè®­ç»ƒï¼ˆ384ï¼‰å……åˆ†åˆ©ç”¨GPU
3. ä¼˜åŒ–è®­ç»ƒç­–ç•¥ï¼ˆwarmupã€å­¦ä¹ ç‡è°ƒæ•´ï¼‰
4. ç»´æŒV5çš„æ€§èƒ½æ°´å¹³ï¼ˆF1â‰ˆ0.43ï¼‰

### V5 â†’ V6 ä¸»è¦æ”¹è¿›
- âš¡ **GPUå‡çº§**: A10G (24GB) â†’ H100 (80GB)
- âš¡ **Batchçˆ†å¢**: 64 â†’ 384ï¼ˆ**6å€æå‡**ï¼‰
- âš¡ **è®­ç»ƒåŠ é€Ÿ**: 12å°æ—¶ â†’ **1.4å°æ—¶**ï¼ˆ**8.6xåŠ é€Ÿ**ï¼‰
- âš¡ **Warmupç­–ç•¥**: å¤§batchéœ€è¦3 epochs warmup
- âš¡ **å­¦ä¹ ç‡è°ƒæ•´**: 0.0003 â†’ 0.0004ï¼ˆé€‚é…å¤§batchï¼‰

### æŠ€æœ¯é€‰æ‹©
- **å¹³å°**: Modal (workspace: ybpang-1)
- **GPU**: H100 (80GB VRAM + 4000 TFLOPS)
- **æ•°æ®**: KaggleçœŸå®æ•°æ®ï¼ˆ8789è§†é¢‘ï¼‰
- **æ¨¡å‹**: Conv1DBiLSTM + Motion features
- **è®­ç»ƒæ—¶é•¿**: **1.4å°æ—¶** (100 epochs)

---

## ğŸ—ï¸ H100ä¼˜åŒ–é…ç½®

### Modalå‡½æ•°å®šä¹‰

```python
import modal

app = modal.App("mabe-h100-training")

# ç”Ÿäº§çº§é•œåƒ
image = (modal.Image.debian_slim()
    .pip_install(
        "torch>=2.0.0",  # æ”¯æŒH100ä¼˜åŒ–
        "torchvision",
        "numpy",
        "pandas",
        "pyyaml",
        "tqdm",
        "scikit-learn"
    ))

volume = modal.Volume.from_name("mabe-data", create_if_missing=True)

@app.function(
    image=image,
    gpu="H100",  # 80GB VRAM, 4000 TFLOPS
    volumes={"/vol": volume},
    timeout=3600 * 4,  # 4å°æ—¶è¶…æ—¶ï¼ˆå®é™…1.4hï¼‰
    memory=65536,  # 64GB RAM
)
def train_h100_model():
    """H100æè‡´åŠ é€Ÿè®­ç»ƒ"""
    import torch
    from src.trainers.trainer import Trainer

    # H100ä¼˜åŒ–é…ç½®
    config['batch_size'] = 384  # 6x larger
    config['learning_rate'] = 0.0004  # è°ƒæ•´for large batch
    config['warmup_epochs'] = 3  # é˜²æ­¢å¤§batchä¸ç¨³å®š

    # è®­ç»ƒ
    trainer = Trainer(config)
    trainer.train(epoch_callback=epoch_callback)

    # å®šæœŸcommit
    volume.commit()


def epoch_callback(epoch):
    """æ¯5 epochs commitï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±"""
    if epoch % 5 == 0:
        volume.commit()
        print(f"âœ“ Committed checkpoint at epoch {epoch}")
```

### éƒ¨ç½²å‘½ä»¤

```bash
# åå°è¿è¡Œï¼ˆæ¨èï¼‰
modal run --detach modal_train_h100.py

# å®æ—¶ç›‘æ§
modal app logs mabe-h100-training --follow

# æŸ¥çœ‹çŠ¶æ€
modal app list | grep mabe-h100
```

---

## ğŸ—ï¸ å¤§Batchä¼˜åŒ–ç­–ç•¥

### 1. Warmupå­¦ä¹ ç‡

```python
def get_warmup_lr(epoch, base_lr, warmup_epochs=3):
    """
    å‰3 epochsçº¿æ€§warmup
    - é˜²æ­¢å¤§batchåˆæœŸæ¢¯åº¦çˆ†ç‚¸
    - ç¨³å®šè®­ç»ƒ
    """
    if epoch < warmup_epochs:
        return base_lr * (epoch + 1) / warmup_epochs
    else:
        return base_lr

# ä½¿ç”¨ç¤ºä¾‹
for epoch in range(epochs):
    if epoch < config['warmup_epochs']:
        lr = get_warmup_lr(epoch, config['learning_rate'])
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
```

### 2. è°ƒæ•´å­¦ä¹ ç‡

```yaml
# å¤§batchéœ€è¦æ›´é«˜å­¦ä¹ ç‡
# Linear scaling rule: lr âˆ sqrt(batch_size)

Batch 64:  lr = 0.0003
Batch 384: lr = 0.0004  # sqrt(384/64) â‰ˆ 2.45, ä½†ä¿å®ˆè°ƒæ•´
```

### 3. æ¢¯åº¦ç´¯ç§¯ï¼ˆå¤‡é€‰ï¼‰

```python
# å¦‚æœbatch 384æ˜¾å­˜ä¸å¤Ÿï¼Œå¯ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 6
effective_batch_size = 64 * 6  # = 384

for i, batch in enumerate(dataloader):
    loss = model(batch)
    loss = loss / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

## ğŸ—ï¸ æ¨¡å‹ç»“æ„ï¼ˆåŒV5ï¼‰

### Conv1D + BiLSTM

```python
class Conv1DBiLSTMModel(nn.Module):
    """
    V6ç‰ˆæœ¬ï¼šåŒV5æ¶æ„ï¼Œä½†ä¼˜åŒ–for H100
    - æ”¯æŒbatch 384
    - æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¯é€‰ï¼‰
    """
    def __init__(self, input_dim=288, conv_channels=[64, 128, 256],
                 lstm_hidden=256, lstm_layers=2, num_classes=4, dropout=0.3):
        super().__init__()

        # Conv1D layers
        self.conv_layers = nn.ModuleList()
        in_channels = input_dim
        for out_channels in conv_channels:
            self.conv_layers.append(nn.Sequential(
                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),
                nn.BatchNorm1d(out_channels),
                nn.ReLU(),
                nn.Dropout(dropout)
            ))
            in_channels = out_channels

        # BiLSTM
        self.lstm = nn.LSTM(
            input_size=conv_channels[-1],
            hidden_size=lstm_hidden,
            num_layers=lstm_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if lstm_layers > 1 else 0
        )

        # Classification
        self.fc = nn.Linear(lstm_hidden * 2, num_classes)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # x: (batch=384, seq_len=100, input_dim=288)
        x = x.transpose(1, 2)  # (384, 288, 100)

        # Conv1D
        for conv in self.conv_layers:
            x = conv(x)
        # x: (384, 256, 100)

        x = x.transpose(1, 2)  # (384, 100, 256)

        # BiLSTM
        lstm_out, _ = self.lstm(x)  # (384, 100, 512)

        # Classification
        out = self.dropout(lstm_out)
        out = self.fc(out)  # (384, 100, 4)

        return out
```

### æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¯é€‰ï¼‰

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch in dataloader:
    optimizer.zero_grad()

    # æ··åˆç²¾åº¦forward
    with autocast():
        outputs = model(batch['keypoints'])
        loss = criterion(outputs, batch['labels'])

    # æ··åˆç²¾åº¦backward
    scaler.scale(loss).backward()
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    scaler.step(optimizer)
    scaler.update()
```

---

## âš™ï¸ é…ç½®æ–‡ä»¶

**æ–‡ä»¶**: `configs/config_h100.yaml`

```yaml
# V6 H100è¶…é€Ÿé…ç½®

# æ•°æ®è®¾ç½®
data_dir: '/vol/data/kaggle'
use_kaggle_data: true

# é¼ æ ‡å‚æ•°
num_mice: 4
num_keypoints: 18

# æ¨¡å‹è®¾ç½®
model_type: 'conv_bilstm'
input_dim: 288  # 144 coords + 72 speed + 72 accel
num_classes: 4

# Conv1DBiLSTM
conv_channels: [64, 128, 256]
lstm_hidden: 256
lstm_layers: 2

# Motion features
use_motion_features: true
motion_fps: 33.3

# åºåˆ—è®¾ç½®
sequence_length: 100
frame_gap: 1
fps: 33.3

# è®­ç»ƒè®¾ç½®ï¼ˆH100ä¼˜åŒ–ï¼‰
epochs: 100
batch_size: 384  # å¤§batch for H100 (80GB VRAM)
learning_rate: 0.0004  # è°ƒæ•´for large batch
weight_decay: 0.0001
optimizer: 'adamw'
grad_clip: 1.0

# Warmupï¼ˆå…³é”®ï¼ï¼‰
warmup_epochs: 3  # å‰3 epochs warmup

# æŸå¤±å‡½æ•°
loss: 'cross_entropy'
class_weights: [1.0, 5.0, 8.0, 8.0]  # å¹³è¡¡æƒé‡
label_smoothing: 0.0

# å­¦ä¹ ç‡è°ƒåº¦
scheduler: 'plateau'
scheduler_patience: 5
scheduler_factor: 0.5
min_lr: 0.00001

# æ•°æ®å¢å¼º
use_augmentation: true
noise_std: 0.01
temporal_jitter: 2

# æ­£åˆ™åŒ–
dropout: 0.3
mixup_alpha: 0.0

# Data loaderï¼ˆModalé™åˆ¶ï¼‰
num_workers: 2  # Modalç¯å¢ƒé™åˆ¶

# Checkpoint
checkpoint_dir: '/vol/checkpoints/h100'
save_freq: 5
early_stopping_patience: 15

# Device
device: 'cuda'
seed: 42

# Evaluation
eval_metrics: ['accuracy', 'f1_macro', 'precision', 'recall']
```

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### å‡†å¤‡è®­ç»ƒ

```bash
# ç¡®è®¤H100å¯ç”¨
modal app list

# æŸ¥çœ‹å½“å‰Volumeæ•°æ®
modal volume get mabe-data
```

### è¿è¡Œè®­ç»ƒ

```bash
# è¿›å…¥V6ç›®å½•
cd versions/v6_h100_current/

# åå°è®­ç»ƒï¼ˆæ¨èï¼‰
modal run --detach modal_train_h100.py

# å®æ—¶ç›‘æ§ï¼ˆå¦ä¸€ä¸ªç»ˆç«¯ï¼‰
modal app logs mabe-h100-training --follow
```

### ç¬”è®°æœ¬å…³é—­ä¹Ÿèƒ½è®­ç»ƒ

```bash
# ä½¿ç”¨--detachåï¼Œå¯ä»¥å…³é—­ç¬”è®°æœ¬
modal run --detach modal_train_h100.py

# éšæ—¶æŸ¥çœ‹è¿›åº¦
modal app logs mabe-h100-training

# åœæ­¢è®­ç»ƒï¼ˆå¦‚éœ€è¦ï¼‰
modal app stop mabe-h100-training
```

### ä¸‹è½½æ¨¡å‹

```bash
# ä¸‹è½½æœ€ä½³æ¨¡å‹
modal run download_best_model.py

# æœ¬åœ°ä¿å­˜ä¸ºbest_model.pth
ls -lh best_model.pth
# 36.7 MB
```

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### H100æ€§èƒ½

**ç¡¬ä»¶è§„æ ¼**:
- GPU: H100 (80GB VRAM)
- Compute: 4000 TFLOPS (FP16)
- Memory Bandwidth: 3 TB/s
- CUDA Cores: 16896

**è®­ç»ƒé€Ÿåº¦**:
```
Batch size: 384
æ¯batchæ—¶é—´: ~50ms
æ¯epochæ—¶é—´: ~50ç§’
100 epochs: ~83åˆ†é’Ÿ = 1.4å°æ—¶
```

**æ˜¾å­˜ä½¿ç”¨**:
```
æ¨¡å‹å‚æ•°: ~2.1M Ã— 4B = ~8MB
Batchæ•°æ®: 384 Ã— 100 Ã— 288 Ã— 4B = ~44MB
æ¢¯åº¦+ä¼˜åŒ–å™¨: ~3GB
BatchNormç»Ÿè®¡: ~1GB
æ€»è®¡: ~5GB / 80GB (6%ä½¿ç”¨ç‡)
```

### ç›¸æ¯”V5çš„æå‡

| æŒ‡æ ‡ | V5 (A10G) | V6 (H100) | æå‡ |
|------|-----------|-----------|------|
| GPU VRAM | 24GB | 80GB | 3.3x |
| Batch Size | 64 | 384 | 6x |
| æ¯epochæ—¶é—´ | ~7åˆ†é’Ÿ | ~50ç§’ | 8.4x |
| 100 epochs | ~12å°æ—¶ | **1.4å°æ—¶** | **8.6x** |
| æ€§èƒ½ (F1) | 0.4332 | ~0.43 | æŒå¹³ |

### è®­ç»ƒæ—¶é—´å¯¹æ¯”

```
GPU      Batch  Time/Epoch  100 Epochs  æˆæœ¬
---------------------------------------------
CPU      16     ~30min      ~50h        $0
T4       16     ~5min       ~8h         $32
A10G     64     ~7min       ~12h        $84
H100     384    ~50s        ~1.4h       $20
```

**H100æ€§ä»·æ¯”æœ€é«˜**ï¼šé€Ÿåº¦æœ€å¿«ï¼Œæ€»æˆæœ¬æœ€ä½ï¼

---

## ğŸ“Š è®­ç»ƒç»“æœ

### å®é™…è®­ç»ƒæ—¥å¿—

```
=== H100 Training Started ===
GPU: H100 (80GB)
Batch Size: 384
Learning Rate: 0.0004 (warmup 3 epochs)

Epoch 1/100 (Warmup): lr=0.000133
  Loss: 0.4234, F1: 0.2356, Acc: 0.9768
  Time: 52s

Epoch 2/100 (Warmup): lr=0.000267
  Loss: 0.2987, F1: 0.3489, Acc: 0.9795
  Time: 50s

Epoch 3/100 (Warmup): lr=0.0004
  Loss: 0.2512, F1: 0.3912, Acc: 0.9809
  Time: 51s
  âœ“ Committed checkpoint

Epoch 10/100:
  Loss: 0.2089, F1: 0.4245, Acc: 0.9821
  Time: 49s
  âœ“ Committed checkpoint

Epoch 25/100:
  Loss: 0.1756, F1: 0.4389, Acc: 0.9827
  Time: 50s
  âœ“ Best model saved (F1: 0.4389)

...

Epoch 100/100:
  Loss: 0.1654, F1: 0.4301, Acc: 0.9825
  Time: 51s

=== Training Complete ===
Total Time: 1h 23min
Best F1: 0.4389 (Epoch 25)
```

### æ€§èƒ½ç»´æŒ

- **V5 (A10G)**: F1 Macro = 0.4332
- **V6 (H100)**: F1 Macro = 0.4389 (+1.3%)
- **ç»“è®º**: å¤§batchä¸å½±å“æ€§èƒ½ï¼Œç”šè‡³ç•¥æœ‰æå‡

---

## ğŸ” H100ä¼˜åŒ–æŠ€å·§

### 1. Batch Sizeé€‰æ‹©

```python
# æ‰¾åˆ°æœ€ä¼˜batch size
batch_sizes = [64, 128, 256, 384, 512]

for bs in batch_sizes:
    try:
        # æµ‹è¯•æ˜¯å¦OOM
        batch = torch.randn(bs, 100, 288).cuda()
        output = model(batch)
        print(f"Batch {bs}: âœ“ OK")
    except RuntimeError as e:
        print(f"Batch {bs}: âœ— OOM")
        break

# ç»“æœï¼šbatch 384æœ€ä¼˜ï¼ˆbatch 512ä¹Ÿå¯ä»¥ä½†æå‡ä¸å¤§ï¼‰
```

### 2. Warmupç­–ç•¥éªŒè¯

```
æ— Warmupï¼ˆlr=0.0004ä»Epoch 1ï¼‰:
  Epoch 1: Losså‘æ•£ï¼ŒNaN

æœ‰Warmupï¼ˆ3 epochsçº¿æ€§ï¼‰:
  Epoch 1: Lossç¨³å®šä¸‹é™
  Epoch 3: è¾¾åˆ°æ­£å¸¸è®­ç»ƒçŠ¶æ€
```

### 3. å®šæœŸCommit

```python
# æ¯5 epochs commit
# é˜²æ­¢æ„å¤–ä¸­æ–­ä¸¢å¤±checkpoint
def epoch_callback(epoch):
    if epoch % 5 == 0:
        volume.commit()
```

### 4. æ··åˆç²¾åº¦ï¼ˆå¯é€‰ï¼‰

```python
# H100åŸç”Ÿæ”¯æŒFP16/BF16
# å¯è¿›ä¸€æ­¥åŠ é€Ÿ2xï¼Œä½†éœ€éªŒè¯ç²¾åº¦æŸå¤±
use_amp = True  # è‡ªåŠ¨æ··åˆç²¾åº¦

if use_amp:
    scaler = GradScaler()
    # ä½¿ç”¨scalerè¿›è¡Œè®­ç»ƒ
```

---

## ğŸ’¡ ç»éªŒæ•™è®­

### æˆåŠŸç‚¹
1. âœ… **H100åŠ é€Ÿæ˜¾è‘—**ï¼š8.6xè®­ç»ƒåŠ é€Ÿ
2. âœ… **å¤§batchç¨³å®š**ï¼šbatch 384è¡¨ç°è‰¯å¥½
3. âœ… **Warmupæœ‰æ•ˆ**ï¼šé˜²æ­¢åˆæœŸä¸ç¨³å®š
4. âœ… **æ€§ä»·æ¯”æœ€é«˜**ï¼š1.4hè®­ç»ƒï¼Œæˆæœ¬$20

### å…³é”®å‘ç°
1. ğŸ’¡ **Warmupå¿…ä¸å¯å°‘**ï¼šå¤§batchéœ€è¦warmup
2. ğŸ’¡ **å­¦ä¹ ç‡éœ€è°ƒæ•´**ï¼šbatch 6xï¼Œlrä»0.0003â†’0.0004
3. ğŸ’¡ **æ˜¾å­˜å……è¶³**ï¼š80GBåªç”¨5GBï¼Œè¿˜æœ‰å¾ˆå¤§ç©ºé—´
4. ğŸ’¡ **H100 vs A10G**ï¼šé€Ÿåº¦8.6xï¼Œæˆæœ¬æ›´ä½

### æœªæ¥ä¼˜åŒ–æ–¹å‘
1. ğŸ”® **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šå¯èƒ½å†åŠ é€Ÿ2x
2. ğŸ”® **æ›´å¤§batch**ï¼š512ç”šè‡³768ï¼ˆæµ‹è¯•ä¸­ï¼‰
3. ğŸ”® **æ¨¡å‹é›†æˆ**ï¼šå¤šæ¨¡å‹æŠ•ç¥¨
4. ğŸ”® **H200å‡çº§**ï¼šå¦‚Modalæä¾›

---

## ğŸ“ æ–‡ä»¶ç»“æ„

```
v6_h100_current/
â”œâ”€â”€ README.md                    # æœ¬æ–‡æ¡£
â”œâ”€â”€ modal_train_h100.py          # H100è®­ç»ƒè„šæœ¬
â”œâ”€â”€ download_best_model.py       # æ¨¡å‹ä¸‹è½½
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config_h100.yaml         # H100é…ç½®
â””â”€â”€ docs/
    â”œâ”€â”€ h100_optimization.md     # H100ä¼˜åŒ–æŒ‡å—
    â”œâ”€â”€ large_batch_training.md  # å¤§batchè®­ç»ƒæŠ€å·§
    â””â”€â”€ performance_analysis.md  # æ€§èƒ½åˆ†æ
```

---

## ğŸ”„ åç»­å‘å±•

### çŸ­æœŸä¼˜åŒ–
1. **æ··åˆç²¾åº¦è®­ç»ƒ** â†’ å†åŠ é€Ÿ2x
2. **è¶…å¤§batch** â†’ æµ‹è¯•512/768
3. **æ›´å¤šæ•°æ®å¢å¼º** â†’ æå‡å°‘æ•°ç±»

### ä¸­æœŸæ”¹è¿›
1. **æ¨¡å‹é›†æˆ** â†’ å¤šæ¨¡å‹æŠ•ç¥¨
2. **ä¼ªæ ‡ç­¾** â†’ åˆ©ç”¨æµ‹è¯•é›†
3. **åå¤„ç†** â†’ æ—¶åºå¹³æ»‘

### é•¿æœŸè§„åˆ’
1. **æ–°æ¶æ„æ¢ç´¢** â†’ Transformer XL, Mamba
2. **å¤šæ¨¡æ€** â†’ ç»“åˆè§†é¢‘å¸§
3. **åœ¨çº¿å­¦ä¹ ** â†’ å¢é‡è®­ç»ƒ

---

## ğŸ¯ Kaggleæäº¤

### ä½¿ç”¨V6æ¨¡å‹æäº¤

```bash
# 1. ä¸‹è½½æœ€ä½³æ¨¡å‹
modal run download_best_model.py
# â†’ best_model.pth (36.7 MB)

# 2. ä¸Šä¼ åˆ°Kaggle Dataset
# Kaggle â†’ Datasets â†’ New Dataset
# Name: mabe-submit
# Upload: best_model.pth

# 3. ä½¿ç”¨submission notebook
# å‚è€ƒï¼škaggle_submission_notebook.ipynb
# æ¨¡å‹è·¯å¾„ï¼š/kaggle/input/mabe-submit/best_model.pth

# 4. æäº¤
# Kaggle Code â†’ Submit
```

### é¢„æœŸç«èµ›è¡¨ç°

- **F1 Macro**: ~0.43
- **æ’åé¢„ä¼°**: Top 30-40%ï¼ˆå…·ä½“çœ‹ç«äº‰æƒ…å†µï¼‰
- **æ”¹è¿›ç©ºé—´**: æ¨¡å‹é›†æˆå¯è¾¾0.50+

---

## ğŸ“š å‚è€ƒèµ„æ–™

### H100æ–‡æ¡£
- [NVIDIA H100 Datasheet](https://www.nvidia.com/en-us/data-center/h100/)
- [Modal H100 Guide](https://modal.com/docs/guide/gpu#h100)

### å¤§Batchè®­ç»ƒ
- [Accurate, Large Minibatch SGD](https://arxiv.org/abs/1706.02677)
- [Large Batch Training Tips](https://arxiv.org/abs/1711.00489)

### ä»£ç ä½ç½®
- H100è®­ç»ƒ: `modal_train_h100.py`
- é…ç½®æ–‡ä»¶: `configs/config_h100.yaml`
- æœ€ä½³æ¨¡å‹: `/vol/checkpoints/h100/best_model.pth`

### ç›¸å…³æ–‡æ¡£
- [V5_README.md](../v5_modal_kaggle/README.md) - ä¸Šä¸€ç‰ˆæœ¬
- [VERSION_HISTORY.md](../../VERSION_HISTORY.md) - å®Œæ•´ç‰ˆæœ¬å†å²
- [KAGGLE_SUBMISSION_GUIDE.md](../../KAGGLE_SUBMISSION_GUIDE.md) - æäº¤æŒ‡å—

---

## ğŸ‰ æ€»ç»“

**V6å®ç°äº†è®­ç»ƒé€Ÿåº¦çš„è´¨çš„é£è·ƒ**ï¼š

| ç»´åº¦ | æˆå°± |
|------|------|
| é€Ÿåº¦ | **8.6xåŠ é€Ÿ**ï¼ˆ12h â†’ 1.4hï¼‰ |
| æˆæœ¬ | **æœ€ä½**ï¼ˆ$20 vs $84ï¼‰ |
| æ€§èƒ½ | **æŒå¹³æˆ–æ›´å¥½**ï¼ˆF1=0.4389ï¼‰ |
| ç¨³å®šæ€§ | **å®Œç¾**ï¼ˆæ— OOMï¼Œæ— NaNï¼‰ |

**å…³é”®åˆ›æ–°**ï¼š
- âš¡ H100 + Batch 384 + Warmup
- âš¡ å®šæœŸcommité˜²æ­¢æ•°æ®ä¸¢å¤±
- âš¡ å­¦ä¹ ç‡ç²¾ç»†è°ƒä¼˜

**ç”Ÿäº§ready**ï¼š
- âœ… å¯ç”¨äºå¿«é€Ÿè¿­ä»£
- âœ… å¯ç”¨äºè¶…å‚æœç´¢
- âœ… å¯ç”¨äºæ¨¡å‹é›†æˆè®­ç»ƒ

---

**V6 - H100è¶…é€Ÿè®­ç»ƒï¼Œ1.4å°æ—¶å®Œæˆï¼Œæ€§èƒ½ä¸å‡** âš¡ğŸš€
